{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des informations/résultats d'entrainement\n",
    "df_train = pd.DataFrame(y_train, columns=['y_train'])\n",
    "df_train['y_eval'] = classifier.predict(X_train)\n",
    "df_train['y_eval_proba'] = classifier.predict_proba(X_train)[:, 1]  # Probabilité d'avoir un \"événement\" (arrêté)\n",
    "df_train.index = dates_train\n",
    "df_train.sort_index(inplace=True)\n",
    "\n",
    "# Création des informations/résultats de test\n",
    "df_pred = pd.DataFrame(y_test, columns=['y_test'])\n",
    "df_pred['y_pred'] = classifier.predict(X_test)\n",
    "df_pred['y_pred_proba'] = classifier.predict_proba(X_test)[:, 1]  # Probabilité d'avoir un \"événement\" (arrêté)\n",
    "df_pred.index = dates_test\n",
    "df_pred.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "# Graphique de visualisation des résultats sur la période d'entrainement\n",
    "time = df_train.index\n",
    "actual = df_train['y_train']\n",
    "predicted = df_train['y_eval']\n",
    "probabilities = df_train['y_eval_proba']\n",
    "\n",
    "# Time Series of Actual vs. Predicted\n",
    "fig = go.Figure()\n",
    "# lines+markers\n",
    "fig.add_trace(go.Scatter(x=time, y=actual, mode='markers', name='Actual', line=dict(width=2)))\n",
    "fig.add_trace(go.Scatter(x=time, y=predicted, mode='markers', name='Predicted', marker=dict(symbol='circle-open-dot'), line=dict(width=1, dash='dot')))\n",
    "fig.add_trace(go.Scatter(x=time, y=probabilities, mode='markers', name='Predicted Probability', marker=dict(symbol='diamond-open')))\n",
    "fig.update_layout(title='Time Series of Observed vs. Evaluated',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Outcome',\n",
    "                legend=dict(\n",
    "                    x=0.5,\n",
    "                    y=-0.3,\n",
    "                    orientation='h',\n",
    "                    xanchor='center',\n",
    "                    yanchor='top'\n",
    "    )\n",
    "                 ,\n",
    "                 height=500)\n",
    "fig.show()\n",
    "\n",
    "# Graphique de visualisation des résultats sur la période de test\n",
    "time = df_pred.index\n",
    "actual = df_pred['y_test']\n",
    "predicted = df_pred['y_pred']\n",
    "probabilities = df_pred['y_pred_proba']\n",
    "\n",
    "# Time Series of Actual vs. Predicted\n",
    "fig = go.Figure()\n",
    "# lines+markers\n",
    "fig.add_trace(go.Scatter(x=time, y=actual, mode='markers', name='Actual', line=dict(width=2)))\n",
    "fig.add_trace(go.Scatter(x=time, y=predicted, mode='markers', name='Predicted', marker=dict(symbol='circle-open-dot'), line=dict(width=1, dash='dot')))\n",
    "fig.add_trace(go.Scatter(x=time, y=probabilities, mode='markers', name='Predicted Probability', marker=dict(symbol='diamond-open')))\n",
    "fig.update_layout(title='Time Series of Actual vs. Predicted',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Outcome',\n",
    "                legend=dict(\n",
    "                    x=0.5,\n",
    "                    y=-0.3,\n",
    "                    orientation='h',\n",
    "                    xanchor='center',\n",
    "                    yanchor='top'\n",
    "    )\n",
    "                 ,\n",
    "                 height=500)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Matrices de confusion sur les périodes d'entrainement et de test\n",
    "cm_train = confusion_matrix(df_train['y_train'], df_train['y_eval'])\n",
    "z_text_train = [[str(y) for y in x] for x in cm_train]  # Convert numbers to string for annotations\n",
    "fig_cm_train = go.Figure(data=go.Heatmap(z=cm_train, x=['Predicted: No', 'Predicted: Yes'], \n",
    "                                         y=['Actual: No', 'Actual: Yes'], \n",
    "                                         colorscale='Blues', text=z_text_train, texttemplate=\"%{text}\"))\n",
    "fig_cm_train.update_layout(title='Confusion Matrix - Training', xaxis_title=\"Predicted Value\", \n",
    "                           yaxis_title=\"Actual Value\", width=600, height=600)\n",
    "fig_cm_train.show()\n",
    "\n",
    "# Confusion Matrix for Testing Data\n",
    "cm_pred = confusion_matrix(df_pred['y_test'], df_pred['y_pred'])\n",
    "z_text_pred = [[str(y) for y in x] for x in cm_pred]  # Convert numbers to string for annotations\n",
    "fig_cm_pred = go.Figure(data=go.Heatmap(z=cm_pred, x=['Predicted: No', 'Predicted: Yes'], \n",
    "                                         y=['Actual: No', 'Actual: Yes'], \n",
    "                                         colorscale='Blues', text=z_text_pred, texttemplate=\"%{text}\"))\n",
    "fig_cm_pred.update_layout(title='Confusion Matrix - Testing', xaxis_title=\"Predicted Value\", \n",
    "                           yaxis_title=\"Actual Value\", width=600, height=600)\n",
    "fig_cm_pred.show()\n",
    "\n",
    "\n",
    "\n",
    "# Courbes ROC sur les périodes d'entrainement et de test\n",
    "\n",
    "# Compute ROC curve and ROC area - training\n",
    "fpr1, tpr1, thresholds1 = roc_curve(df_train['y_train'], df_train['y_eval_proba'])\n",
    "roc_auc1 = roc_auc_score(df_train['y_train'], df_train['y_eval_proba'])\n",
    "\n",
    "# Plotting ROC curve - training\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(x=fpr1, y=tpr1, mode='lines', name='ROC curve (area = %0.2f)' % roc_auc1,\n",
    "                                   line=dict(color='darkorange', width=2)))\n",
    "fig1.add_shape(type='line', line=dict(dash='dash'),\n",
    "                        x0=0, x1=1, y0=0, y1=1)\n",
    "fig1.update_layout(title='Receiver Operating Characteristic (ROC) - Training',\n",
    "                            xaxis_title='False Positive Rate',\n",
    "                            yaxis_title='True Positive Rate',\n",
    "                            legend=dict(x=0.1, y=0.9),\n",
    "                            autosize=False,\n",
    "                            width=600,\n",
    "                            height=600)\n",
    "fig1.show()\n",
    "\n",
    "# Compute ROC curve and ROC area - testing\n",
    "fpr2, tpr2, thresholds2 = roc_curve(df_pred['y_test'], df_pred['y_pred_proba'])\n",
    "roc_auc2 = roc_auc_score(df_pred['y_test'], df_pred['y_pred_proba'])\n",
    "\n",
    "# Plotting ROC curve - testing\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(x=fpr2, y=tpr2, mode='lines', name='ROC curve (area = %0.2f)' % roc_auc2,\n",
    "                                   line=dict(color='darkorange', width=2)))\n",
    "fig2.add_shape(type='line', line=dict(dash='dash'),\n",
    "                        x0=0, x1=1, y0=0, y1=1)\n",
    "fig2.update_layout(title='Receiver Operating Characteristic (ROC) - Testing',\n",
    "                            xaxis_title='False Positive Rate',\n",
    "                            yaxis_title='True Positive Rate',\n",
    "                            legend=dict(x=0.1, y=0.9),\n",
    "                            autosize=False,\n",
    "                            width=600,\n",
    "                            height=600)\n",
    "fig2.show()\n",
    "\n",
    "\n",
    "\n",
    "# Informations complémentaiures sur les périodes d'entrainement et de test\n",
    "\n",
    "# Confidence of Predictions\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Histogram(x=df_train['y_eval_proba'], name='Train Probabilities'))\n",
    "fig1.add_trace(go.Histogram(x=df_pred['y_pred_proba'], name='Test Probabilities'))\n",
    "fig1.update_layout(title='Confidence of Predictions',\n",
    "                   xaxis_title='Predicted Probability',\n",
    "                   yaxis_title='Count',\n",
    "                   barmode='overlay')\n",
    "fig1.update_traces(opacity=0.75)\n",
    "fig1.show()\n",
    "\n",
    "# Feature Importances\n",
    "try:\n",
    "    importances = classifier.feature_importances_\n",
    "    sorted_indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    fig2 = go.Figure()\n",
    "    fig2.add_trace(go.Bar(x=df.iloc[:,lfeat].columns[sorted_indices], y=importances[sorted_indices]))\n",
    "    fig2.update_layout(title='Feature Importances',\n",
    "                       xaxis_title='Features',\n",
    "                       yaxis_title='Importance')\n",
    "    fig2.show()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Overfitting (comparing AUC scores for training and testing sets)\n",
    "auc_train = roc_auc_score(df_train['y_train'], df_train['y_eval_proba'])\n",
    "auc_test = roc_auc_score(df_pred['y_test'], df_pred['y_pred_proba'])\n",
    "\n",
    "fig3 = go.Figure()\n",
    "fig3.add_trace(go.Bar(x=['Train', 'Test'], y=[auc_train, auc_test]))\n",
    "fig3.update_layout(title='AUC Scores for Train and Test Sets',\n",
    "                   xaxis_title='Dataset',\n",
    "                   yaxis_title='AUC Score')\n",
    "fig3.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
